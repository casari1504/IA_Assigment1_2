{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584b150b-8cec-4db4-a1a8-85b4866b6dfa",
   "metadata": {},
   "source": [
    "# DISCLAIMER\n",
    "The `random_state` is set to 42 because it is a common convention choice in tutorials and assignments (a \"fun convention\" from *The Hitchhiker's Guide to the Galaxy*). You can pick any integer, but different integers produce different sequence of randomness so your results will not be identical across different seeds. Using a fixed `random_state` ensures reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fcbda0-57fc-4faf-99d5-7775b455a7d8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1afd4f37-c368-4974-b820-ce91364a9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,cross_validate, StratifiedKFold,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report,precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729978fd-af41-4daa-bba0-9a08e2748501",
   "metadata": {},
   "source": [
    "### Load wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6c3104-6bbc-4b0a-ac1d-5123b36b5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables X, Y and df\n",
    "wine = datasets.load_wine()\n",
    "X=wine.data\n",
    "Y=wine.target\n",
    "df=pd.DataFrame(X,columns=wine.feature_names)\n",
    "df.head()\n",
    "\n",
    "#changing name of troublesome column\n",
    "i = wine.feature_names.index('od280/od315_of_diluted_wines')\n",
    "wine.feature_names[i] = 'ratio_of_diluted_wines'\n",
    "df = df.rename(columns={'od280/od315_of_diluted_wines': 'ratio_of_diluted_wines'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea55769a-cdbe-4ab4-b408-4957b2ffe2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a MLP (Multi Layer Perceptron) model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fac922-f0a0-43b1-837b-8543f7df7d08",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Baseline model: MLP classifier with default setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c168a662-e60f-4a05-ad3e-c468bf28fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers:  3\n"
     ]
    }
   ],
   "source": [
    "default_model = MLPClassifier(max_iter = 1400)\n",
    "default_model.fit(X_train, Y_train) #finding weights\n",
    "\n",
    "layers = default_model.n_layers_\n",
    "print(\"Layers: \", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c3a3b6b-91a6-4f57-ba4a-86699e7a35fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.88888889 0.94444444 0.44444444 0.27777778 0.11111111 1.\n",
      " 0.38888889 0.61111111 1.         1.        ]\n",
      "Cross validation f1 value: 0.9504315655296047\n",
      "Cross validation mean: 0.6666666666666666\n",
      "Cross validation standard deviation:  0.32394177193585\n",
      "Test accuracy:  0.75 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_default = default_model.predict(X_test)\n",
    "accuracy_1 = accuracy_score(Y_test, predictions_default)\n",
    "scores_1 = cross_val_score(default_model, X, Y, cv=10, scoring=\"accuracy\")\n",
    "baseline_scores = cross_val_score(baseline_mlp,X, Y,cv=10,scoring='f1_weighted')\n",
    "\n",
    "print(\"Cross validation values:\\n\", scores_1)\n",
    "print(\"Cross validation f1 value:\", np.mean(baseline_scores))\n",
    "print(\"Cross validation mean:\", scores_1.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_1.std())\n",
    "print(\"Test accuracy: \", accuracy_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf201b-caab-4f28-b94a-aa2d6e9b5ae4",
   "metadata": {},
   "source": [
    "**QUESTION 1.1**\n",
    "\n",
    "Above is the MLP classifier built using the standard setup from SK-learn, along with the evaluation of its performance. \n",
    "\n",
    "To ensure that the MLP has enough of time (number of iterations) to learn, we increased the `max_iter` beyond its default value. By experimenting different values, starting at 1000 and increasing to 2000, we gradually narrowed down an appropriate iteration limit. In the end we chose 1400 iterations, as this prevented convergence warnings and ensured that the optimizer completed training before reaching the maximum iteration limit.\n",
    "\n",
    "**Performance evaluation**\n",
    "\n",
    "To evaluate the baseline model's generalization, we repeated 10-fold CV five times manually. This could have been done using a for-loop but we chose this simpler approach. The results for each repeat were:\n",
    "1. CV mean: ~0.505, CV standard deviation: ~0.226\n",
    "2. CV mean: ~0.827, CV standard deviation: ~0.176\n",
    "3. CV mean: ~0.528, CV standard deviation: ~0.195\n",
    "4. CV mean: ~0.703 , CV standard deviation: ~0.241\n",
    "5. CV mean: ~0.755 , CV standard deviation: ~0.246\n",
    "\n",
    "Overall CV mean: (0.505 + 0.827 + 0.528 + 0.703 + 0.755) / 5 = 0.6636\n",
    "\n",
    "The default model achieves an overall cross-validation mean of 0.6636 indicating that it is not consistently strong across different subsets of the data and that the performance varies depending on how the data is split. The relatively high standard deviations for each repeat (0.176-0.246) indicate that the model is unstable and fluctuates considerably across folds. The model reaches a test accuracy of 0.388, meaning that the model's performance on a single test split is highly dependent on the split. All these factors suggests that the rules of thumb model's performance is not very reliable and its ability to generalize to unseen data is limited without tuning or improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b646e1-87e7-4a1d-9a02-5a03517927b0",
   "metadata": {},
   "source": [
    "**Applying the standard rules of thumb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1eb27ee9-b72d-4d91-bf28-80db6a7ba16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.55555556 0.27777778 0.44444444 0.44444444 0.27777778 0.38888889\n",
      " 0.33333333 0.77777778 0.35294118 0.47058824]\n",
      "Cross validation mean: 0.43235294117647055\n",
      "Cross validation standard deviation:  0.14214033410033095\n",
      "Test accuracy:  0.3888888888888889\n"
     ]
    }
   ],
   "source": [
    "rule_of_thumb_model = MLPClassifier(hidden_layer_sizes=(8,), max_iter=2000)\n",
    "rule_of_thumb_model.fit(X_train, Y_train)\n",
    "\n",
    "predictions_rule_of_thumb = rule_of_thumb_model.predict(X_test)\n",
    "accuracy_2 = accuracy_score(Y_test, predictions_rule_of_thumb)\n",
    "scores_2 = cross_val_score(rule_of_thumb_model, X, Y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "print(\"Cross validation values:\\n\", scores_2)\n",
    "print(\"Cross validation mean:\", scores_2.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_2.std())\n",
    "print(\"Test accuracy: \", accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ee767-0ed2-427d-8050-3bde11bcd599",
   "metadata": {},
   "source": [
    "**QUESTION 1.2 and 1.2.1**\n",
    "\n",
    "Above is MLP classifier built in accordance with the rules of the thumb.\n",
    "\n",
    "We increased the value of `max_iter` to 2000 iterations to prevent convergence warnings and ensure that the optimizer completed training before reaching the maximum iteration limit.\n",
    "\n",
    "Based on the wine dataset complexity (13 input features and 3 output classes), we applied the standard rules of the thumb for determining an appropriate hidden layer size for MLP classifier. The rules are applied as follows: \n",
    "\n",
    "**Rule 1: Number of neurons somewhere in-between the size of the input and output layer**\n",
    "\n",
    "The input layer has 13 neurons and the output layer has 3 neurons --> 3-13 neurons\n",
    "\n",
    "**Rule 2: Mean input and output**\n",
    "\n",
    "(13 + 3) / 2 = 8 neurons\n",
    "\n",
    "**Rule 3: Less than twice the input layer**\n",
    "\n",
    "Twice the input size is 26, therefore less than 26 --> 1-25 neurons, however this rule is quite broad and not as informative\n",
    "\n",
    "**Rule 4: Two-thirds rule**\n",
    "\n",
    "(2/3 * 13) + 3 ~= 11-12 neurons\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Combining the rules, it is reasonable to evaluate hidden layer sizes in the range 3-12 neurons. Among these, 8 neurons is particularly strong candidate as it is supported by multiple rules, 3 rules in total(rule 1-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f128-3a6e-46fd-9e00-2175e3ff0640",
   "metadata": {},
   "source": [
    "**QUESTION 1.2.2**\n",
    "\n",
    "**Performance evaluation**\n",
    "\n",
    "To evaluate the rules of thumb model's generalization, we repeated 10-fold CV five times manually. This could have been done using a for-loop but we chose this simpler approach. The results for each repeat were:\n",
    "1. CV mean: ~0.377, CV standard deviation: ~0.079\n",
    "2. CV mean: ~0.342, CV standard deviation: ~0.122\n",
    "3. CV mean: ~0.355, CV standard deviation: ~0.117\n",
    "4. CV mean: ~0.336 , CV standard deviation: ~0.124\n",
    "5. CV mean: ~0.537 , CV standard deviation: ~0.309 \n",
    "\n",
    "Overall CV mean: (0.337 + 0.342 + 0.355 + 0.336 + 0.537) / 5 = 0.3814\n",
    "\n",
    "The rules of thumb model achieves an overall cross-validation mean of 0.3814 indicating that it is not consistently strong across different subsets of the data and that the performance varies depending on how the data is split. The relatively high standard deviations for each repeat (0.079-0.309) indicate that the model is unstable and flactuates considerably across folds. The model reaches a test accuracy of 0.388, meaning that the model's performance on a single test split is highly dependent on the split. All these factors suggests that the rules of thumb model's performance is not very robust and its ability to generalize to unseen data is limited without tuning or improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c874a-9a7f-4deb-a697-6e4a48526bfd",
   "metadata": {},
   "source": [
    "**QUESTION 1.3**\n",
    "\n",
    "The best baseline model out of the two previously built models (the default model and the rules-of-thumb model) is the default model based on their performance. The default model has a better overall cross-validation mean of 0.6636 and relatively lower and more consistent standard deviations across repeats (0.176-0.246). In comparison, the rules of thumb model has a lower overall cross-validation mean of 0.3814 and high variability across repeats with standard deviations ranging from 0.079-0.309. Besides, the default model is easier and simpler to set up, whereas the rules-of-thumb model requires precomputing of the number of neurons, making its setup more complex and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dd6ae7d-d4a8-4234-aa4f-c626ed7a959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPARATION RESULTS:\n",
      "{'fit_time': array([0.60001945, 0.63606143, 0.6003058 , 0.68235064, 0.75145149]), 'score_time': array([0.00976014, 0.01174569, 0.01055551, 0.01364017, 0.01164341]), 'test_accuracy': array([0.97222222, 0.97222222, 0.97222222, 1.        , 0.97142857]), 'test_precision_macro': array([0.96969697, 0.96969697, 0.97777778, 1.        , 0.96666667]), 'test_recall_macro': array([0.97619048, 0.97619048, 0.96666667, 1.        , 0.97777778]), 'test_f1_macro': array([0.97178131, 0.97178131, 0.97096189, 1.        , 0.97096189])}\n",
      "\n",
      " UPDATED BASELINE MLP RESULTS:\n",
      "Baseline MLP F1-score (weighted): 0.9766047079701448\n",
      "    \n",
      "\n",
      " PCA TECHNIQUE RESULTS:\n",
      "Original number of features: 13\n",
      "Number of PCA components: 10\n",
      "\n",
      " MLP WITH PCA TECHNIQUE RESULTS:\n",
      "Cross-validated F1-score with PCA: 0.9707983193277311\n",
      "    \n",
      "EVALUATION OF TEST SET RESULTS:\n",
      "Accuracy: 1.0\n",
      "Precision (weighted): 1.0\n",
      "Recall (weighted): 1.0\n",
      "F1-score (weighted): 1.0\n",
      "Confusion matrix:\n",
      " [[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n"
     ]
    }
   ],
   "source": [
    "#Data Standarization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Applying balancing agent\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, Y_train)\n",
    "X_test_final = X_test_scaled\n",
    "y_test_final = Y_test\n",
    "\n",
    "#Testing improvements through cross validation and accuracy\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mlp', MLPClassifier(random_state=42, max_iter=1400))\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate with multiple metrics\n",
    "scores = cross_validate(\n",
    "    pipeline, X, Y,\n",
    "    cv=cv,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    ")\n",
    "print(\"DATA PREPARATION RESULTS:\")\n",
    "print(scores)\n",
    "\n",
    "#Model improvements\n",
    "baseline_mlp = MLPClassifier(random_state=42, max_iter=1400)\n",
    "baseline_scores = cross_val_score(baseline_mlp,\n",
    "                                  X_train_resampled, \n",
    "                                  y_train_resampled,\n",
    "                                  cv=cv,\n",
    "                                  scoring='f1_weighted'\n",
    "                                 )\n",
    "print(\"\\n UPDATED BASELINE MLP RESULTS:\")\n",
    "print(\"Baseline MLP F1-score (weighted):\", np.mean(baseline_scores))\n",
    "\n",
    "#Applying PCA Techinque\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_resampled)\n",
    "X_test_pca = pca.transform(X_test_final)\n",
    "\n",
    "print(\"    \")\n",
    "print(\"\\n PCA TECHNIQUE RESULTS:\")\n",
    "print(\"Original number of features:\", X_train.shape[1])\n",
    "print(\"Number of PCA components:\", X_train_pca.shape[1])\n",
    "\n",
    "#Conducting MLP with PCA\n",
    "mlp_pca = MLPClassifier(hidden_layer_sizes=(50,50), max_iter=1000, random_state=42)\n",
    "\n",
    "scores = cross_val_score(mlp_pca, \n",
    "                         X_train_pca, \n",
    "                         y_train_resampled,\n",
    "                         cv=cv, \n",
    "                         scoring='f1_weighted')\n",
    "\n",
    "print(\"\\n MLP WITH PCA TECHNIQUE RESULTS:\")\n",
    "print(\"Cross-validated F1-score with PCA:\", np.mean(scores))\n",
    "\n",
    "#Evaluation of test set\n",
    "mlp_pca.fit(X_train_pca, y_train_resampled)\n",
    "y_pred = mlp_pca.predict(X_test_pca)\n",
    "\n",
    "print(\"    \")\n",
    "print(\"EVALUATION OF TEST SET RESULTS:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, y_pred))\n",
    "print(\"Precision (weighted):\", precision_score(Y_test, y_pred, average='weighted'))\n",
    "print(\"Recall (weighted):\", recall_score(Y_test, y_pred, average='weighted'))\n",
    "print(\"F1-score (weighted):\", f1_score(Y_test, y_pred, average='weighted'))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a0fd-1425-4650-860f-1f0c7597d16c",
   "metadata": {},
   "source": [
    "**QUESTION 2.1**\n",
    "\n",
    "**Data Preparation**\n",
    "\n",
    "Seeing as how both models are very dependent on how the data is split, one option would be to prepare the data beforehand with thechinques such as normalization or standarization in order to reduce the variance and distances among the different values. Also, since in previous assignments we learned that the classes in the dataset are slighly imbalaced, we introduce a balancing component in an attempt to bias and the F1 score. Finally, to ensure that each fold has the same class distribution as the full dataset, we will introduce the stratification.\n",
    "\n",
    "**Model Improvement**\n",
    "\n",
    "In order to improve the model we will first apply the MLP method on the prepared data in order to set a new, possibly improved, baseline. After that we will apply the PCA technique in order to reduce the dimensionality of the data set and remove the possible redundancies. After that , wi will train the MLP model with the data obtained from the PCA to  improve MLP training speed and reduce overfitting.Finally we evaluate the test set with all the changes applied in order to verify their effects on the model.\n",
    "\n",
    "\n",
    "As seen by de results, the data preparation allowed for a more stable model across every fold, since the scores in the data preparation are very close to 1. This is further confirmed with the weighted F1 of the new baseline model, which is higher than the original one. When applying the PCA technique, we reduce the 13 original features to 10 main components, preserving the variance and obtaining a perfect confussion matrix, which is the definite proof that the improvements on the dataset and the model have worked, not by outreforming the cross validatio test but in the test performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b26e5c00-cbf1-4b3a-b3ed-b737d0fa1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'mlp__activation': 'relu', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100, 50), 'mlp__learning_rate_init': 0.001}\n",
      "\n",
      "Best cross-validated F1-score:\n",
      "0.9888678699729315\n"
     ]
    }
   ],
   "source": [
    "#Grid search \n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [\n",
    "        (50,), (100,), (150,),        # one hidden layer\n",
    "        (50,50), (100,50), (100,100)  # two hidden layers\n",
    "    ],    \n",
    "    'mlp__activation': ['relu', 'tanh'],    \n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01],    \n",
    "    'mlp__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nBest cross-validated F1-score:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ee2d3-4dc1-41a7-82d7-4a06e2948704",
   "metadata": {},
   "source": [
    "**QUESTION 2.2**\n",
    "\n",
    "Above is the first implementation of a grid search. Since there are 178 samples in the dataset, we decidet to work with two layers instead of one like in the first examples given in class. Obviously, this parameters will change as the rest of the questions of the assignment are answered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899d4c9-6269-4d1b-bb1a-89fa0acfeb12",
   "metadata": {},
   "source": [
    "# BELOW ARE CODE COPIED FROM LECTURE SLIDES SO THAT WE DON'T NEED TO WRITE EVERY CODE MANUALLY - THIS IS TO BE REMOVED AFTER FINISHED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0a562-415b-4ae0-a6cb-dad9b79714d1",
   "metadata": {},
   "source": [
    "**Using grid search with cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95e672f4-4ce2-47d5-813b-86517e8e8a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (2,)}\n"
     ]
    }
   ],
   "source": [
    "search_model_1 = MLPClassifier(max_iter = 10000)\n",
    "H_param = {\n",
    "    'hidden_layer_sizes' : [(1,), (2,), (3,), (4,), (5,), (6,), (7,),]\n",
    "}\n",
    "optimal_param = GridSearchCV(search_model_1, H_param)\n",
    "optimal_param.fit(X, Y)\n",
    "print(optimal_param.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d8e1b-03c7-47c1-9673-416109c362e3",
   "metadata": {},
   "source": [
    "**Evaluating the suggested best classfier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "846f7719-1520-44c5-a1ee-b5e9444af065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.33333333 0.27777778 0.88888889 0.27777778 0.38888889 0.61111111\n",
      " 0.38888889 0.61111111 0.23529412 0.47058824]\n",
      "Cross validation mean: 0.44836601307189544\n",
      "Cross validation standard deviation:  0.19272624725802323\n"
     ]
    }
   ],
   "source": [
    "best_model = MLPClassifier(hidden_layer_sizes=(2,), max_iter=10000)\n",
    "scores_best = cross_val_score(best_model, X, Y, cv=10, scoring=\"accuracy\")\n",
    "print(\"Cross validation values:\\n\", scores_best)\n",
    "print(\"Cross validation mean:\", scores_best.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_best.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66508a95-06b0-406a-98c8-e2f69a3d8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.38888889 0.33333333 0.38888889 0.27777778 0.61111111 0.27777778\n",
      " 0.33333333 0.61111111 0.23529412 0.29411765]\n",
      "Cross validation mean: 0.37516339869281046\n",
      "Cross validation standard deviation:  0.12661701341022305\n"
     ]
    }
   ],
   "source": [
    "scores_best = cross_val_score(best_model, X, Y, cv=10, scoring=\"accuracy\")\n",
    "print(\"Cross validation values:\\n\", scores_best)\n",
    "print(\"Cross validation mean:\", scores_best.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_best.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc78b9c-c1d6-43bf-95e1-bed90d3d5780",
   "metadata": {},
   "source": [
    "**Including: hidden layer size and activation function in the grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f1229ba-0878-4ca9-a831-f8452e175dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'logistic', 'hidden_layer_sizes': (7,)}\n"
     ]
    }
   ],
   "source": [
    "search_model_2 = MLPClassifier(max_iter = 10000)\n",
    "H_param = {\n",
    "    'hidden_layer_sizes' : [(1,), (2,), (3,), (4,), (5,), (6,), (7,)],\n",
    "    'activation' : ['identity', 'logistic', 'tanh', 'relu']\n",
    "}\n",
    "optimal_param = GridSearchCV(search_model_2, H_param)\n",
    "optimal_param.fit(X, Y)\n",
    "print(optimal_param.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a2dcaed-58e7-4a5b-992c-1654d2c3e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.88888889 0.38888889 0.38888889 0.38888889 0.38888889 0.38888889\n",
      " 1.         0.38888889 1.         0.47058824]\n",
      "Cross validation mean: 0.569281045751634\n",
      "Cross validation standard deviation:  0.2604179450180081\n"
     ]
    }
   ],
   "source": [
    "model_2 = MLPClassifier(activation='logistic', hidden_layer_sizes=(6,), max_iter=10000)\n",
    "scores_model_2 = cross_val_score(model_2, X, Y, cv=10, scoring=\"accuracy\")\n",
    "print(\"Cross validation values:\\n\", scores_model_2)\n",
    "print(\"Cross validation mean:\", scores_model_2.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_model_2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab0699aa-ba1a-4490-bd02-952e436c4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.38888889 0.33333333 0.38888889 0.38888889 0.55555556 0.38888889\n",
      " 0.38888889 0.33333333 0.41176471 0.29411765]\n",
      "Cross validation mean: 0.38725490196078427\n",
      "Cross validation standard deviation:  0.06590053826204278\n"
     ]
    }
   ],
   "source": [
    "model_3 = MLPClassifier(hidden_layer_sizes=(2,2,2), max_iter=10000)\n",
    "scores_model_3 = cross_val_score(model_3, X, Y, cv=10, scoring=\"accuracy\")\n",
    "print(\"Cross validation values:\\n\", scores_model_3)\n",
    "print(\"Cross validation mean:\", scores_model_3.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_model_3.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f864e730-51f2-4ac7-ac77-f10dbad10325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "model_4 = MLPClassifier(max_iter = 10000)\n",
    "H_param = {\n",
    "    'hidden_layer_sizes' : [(1,), (2,), (3,), (4,), (5,), (6,), (7,)],\n",
    "    'solver' : ['sgd', 'adam', 'lbfgs'],\n",
    "    'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init' : [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "}\n",
    "optimal_param = GridSearchCV(model_4, H_param, n_jobs=-1)\n",
    "optimal_param.fit(X, Y)\n",
    "print(optimal_param.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0adc8f74-cd5d-4a3c-98fe-c3261cc7cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation values:\n",
      " [0.88888889 0.27777778 0.38888889 0.38888889 0.38888889 0.77777778\n",
      " 0.72222222 0.94444444 0.41176471 0.47058824]\n",
      "Cross validation mean: 0.5660130718954248\n",
      "Cross validation standard deviation:  0.22952160192063947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL: .\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "model_5 = MLPClassifier(hidden_layer_sizes=6, learning_rate='adaptive', learning_rate_init=0.05, solver='lbfgs', max_iter=10000)\n",
    "scores_model_5 = cross_val_score(model_5, X, Y, cv=10, scoring=\"accuracy\")\n",
    "print(\"Cross validation values:\\n\", scores_model_5)\n",
    "print(\"Cross validation mean:\", scores_model_5.mean())\n",
    "print(\"Cross validation standard deviation: \", scores_model_5.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26f02c3c-7569-45a8-8376-e3c91e3e659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.95      0.95        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5.fit(X_train, Y_train)\n",
    "predictions_5 = model_5.predict(X_test)\n",
    "print(classification_report(Y_test, predictions_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67894eb5-5134-4c80-a8b7-355b4c1d980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.42      1.00      0.59        15\n",
      "           2       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.42        36\n",
      "   macro avg       0.14      0.33      0.20        36\n",
      "weighted avg       0.17      0.42      0.25        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model_5.fit(X_train, Y_train)\n",
    "predictions_5 = model_5.predict(X_test)\n",
    "print(classification_report(Y_test, predictions_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063f09a-5b59-4b89-b702-31dc6a3123e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
